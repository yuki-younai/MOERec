{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from utils.parser import parse_args\n",
    "from utils.dataloader import Dataloader\n",
    "from utils.utils import config, construct_negative_graph, choose_model, load_mf_model, NegativeGraph\n",
    "from utils.tester import Tester\n",
    "from models.sampler import NegativeSampler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default = 'TaoBao', type = str,\n",
    "                    help = 'Dataset to use')\n",
    "parser.add_argument('--seed', default = 2022, type = int,\n",
    "                    help = 'seed for experiment')\n",
    "parser.add_argument('--embed_size', default = 32, type = int,\n",
    "                    help = 'embedding size for all layer')\n",
    "parser.add_argument('--lr', default = 0.05, type = float,\n",
    "                    help = 'learning rate')\n",
    "parser.add_argument('--weight_decay', default = 8e-8, type = float,\n",
    "                    help = \"weight decay for adam optimizer\")\n",
    "#######################################################################\n",
    "parser.add_argument('--model', default ='moe',type = str,\n",
    "                    help = 'model selection')#dgrec base moe test\n",
    "parser.add_argument('--epoch', default = 1000, type = int,\n",
    "                    help = 'epoch number')\n",
    "parser.add_argument('--patience', default = 10, type = int,\n",
    "                    help = 'early_stop validation')\n",
    "parser.add_argument('--batch_size', default = 2048, type = int,\n",
    "                    help = 'batch size')\n",
    "parser.add_argument('--layers', default = 1, type = int,\n",
    "                    help = 'layer number')\n",
    "parser.add_argument('--gpu', default = 0, type = int,\n",
    "                    help = '-1 for cpu, 0 for gpu:0')\n",
    "parser.add_argument('--k_list', default = [100, 300], type = list,\n",
    "                    help = 'topk evaluation')\n",
    "parser.add_argument('--k', default = 20, type = int,\n",
    "                    help = 'neighbor number in each GNN aggregation')\n",
    "parser.add_argument('--neg_number', default = 4, type = int,\n",
    "                    help = 'negative sampler number for each positive pair')\n",
    "parser.add_argument('--metrics', default = ['recall', 'hit_ratio', 'coverage'])\n",
    "\n",
    "\n",
    "parser.add_argument('--sigma', default = 1.0, type = float,\n",
    "                    help = 'sigma for gaussian kernel')\n",
    "parser.add_argument('--gamma', default = 2.0, type = float,\n",
    "                    help = 'gamma for gaussian kernel')\n",
    "################################################################################\n",
    "parser.add_argument('--category_balance', default = True, type = bool,\n",
    "                    help = 'whether make loss category balance')\n",
    "parser.add_argument('--beta_class', default = 0.9, type = float,\n",
    "                    help = 'class re-balanced loss beta')\n",
    "parser.add_argument('--context_code_dim', default = 32, type = int,\n",
    "                    help = 'interest num')\n",
    "parser.add_argument('--num_context_codes', default = 32, type = int,\n",
    "                    help = 'interest dim')\n",
    "parser.add_argument('--n_experts', default = 5, type = int,\n",
    "                    help = 'n_experts')\n",
    "##########################################################################################\n",
    "parser.add_argument('--wandb_enable', default = True, type = bool,\n",
    "                    help = 'layer number')\n",
    "parser.add_argument('--hidden_size', default = 32, type = int,\n",
    "                        help = 'n_experts')\n",
    "parser.add_argument('--k_experts', default = 2, type = int,\n",
    "                            help = 'n_experts')\n",
    "parser.add_argument('--moe', default = True, type = bool,\n",
    "                            help = 'layer number')\n",
    "################################################################################\n",
    "parser.add_argument('--sub', default = 'rand', type = str,\n",
    "                            help = 'layer number')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136710 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136710/136710 [00:07<00:00, 18215.43it/s]\n",
      "100%|██████████| 2571752/2571752 [00:04<00:00, 568998.94it/s]\n",
      "100%|██████████| 845781/845781 [00:00<00:00, 1012526.82it/s]\n",
      "100%|██████████| 136710/136710 [00:00<00:00, 4188207.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already setting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if args.gpu >= 0 and torch.cuda.is_available():\n",
    "        device = 'cuda:{}'.format(args.gpu)\n",
    "else:\n",
    "        device = 'cpu'\n",
    "device = torch.device(device)\n",
    "args.device = device\n",
    "\n",
    "data = args.dataset\n",
    "dataloader = Dataloader(args, data, device)\n",
    "# NegativeGraphConstructor = NegativeGraph(dataloader.historical_dict)\n",
    "sample_weight = dataloader.sample_weight.to(device)\n",
    "\n",
    "model = choose_model(args, dataloader)\n",
    "model = model.to(device)\n",
    "print(\"model already setting\")\n",
    "opt = torch.optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "[09:17:32] C:\\Users\\Administrator\\DGL_scripts\\release\\win-64\\dgl\\src\\runtime\\cuda\\cuda_device_api.cc:117: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\code\\github\\MOERec\\main2.ipynb 单元格 4\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/github/MOERec/main2.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss_moe\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/code/github/MOERec/main2.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mmoe:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/code/github/MOERec/main2.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         score_pos, score_neg,loss_moe \u001b[39m=\u001b[39m model(graph_pos, graph_neg)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/github/MOERec/main2.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/code/github/MOERec/main2.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         score_pos, score_neg \u001b[39m=\u001b[39m model(graph_pos, graph_neg)\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\code\\github\\MOERec\\models\\models.py:70\u001b[0m, in \u001b[0;36mBaseGraphModel.forward\u001b[1;34m(self, graph_pos, graph_neg)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, graph_pos, graph_neg):\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmoe:\n\u001b[1;32m---> 70\u001b[0m          h,loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_embedding()\n\u001b[0;32m     71\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m          h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_embedding()\n",
      "File \u001b[1;32md:\\code\\github\\MOERec\\models\\models.py:237\u001b[0m, in \u001b[0;36mMOERec.get_embedding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m    236\u001b[0m     h_item \u001b[39m=\u001b[39m layer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph, h, (\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m--> 237\u001b[0m     h_user,muti_int \u001b[39m=\u001b[39m layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph, h, (\u001b[39m'\u001b[39;49m\u001b[39mitem\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrated by\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    238\u001b[0m     \u001b[39m#h_user=self.attention_experts(muti_int)\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m#h_user=muti_int.sum(dim=1)\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     h \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m: h_user, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m: h_item}\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\code\\github\\MOERec\\models\\layers.py:379\u001b[0m, in \u001b[0;36mBasetestLayer.forward\u001b[1;34m(self, graph, h, etype)\u001b[0m\n\u001b[0;32m    377\u001b[0m graph\u001b[39m.\u001b[39mnodes[src]\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feat_src\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 379\u001b[0m     graph\u001b[39m.\u001b[39;49mupdate_all(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategory_aggregation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msub_reduction_item_user, etype \u001b[39m=\u001b[39;49m etype)\n\u001b[0;32m    380\u001b[0m     muti_int\u001b[39m=\u001b[39mgraph\u001b[39m.\u001b[39mnodes[dst]\u001b[39m.\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    381\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\heterograph.py:5110\u001b[0m, in \u001b[0;36mDGLGraph.update_all\u001b[1;34m(self, message_func, reduce_func, apply_node_func, etype)\u001b[0m\n\u001b[0;32m   5108\u001b[0m _, dtid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mmetagraph\u001b[39m.\u001b[39mfind_edge(etid)\n\u001b[0;32m   5109\u001b[0m g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m etype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m[etype]\n\u001b[1;32m-> 5110\u001b[0m ndata \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mmessage_passing(\n\u001b[0;32m   5111\u001b[0m     g, message_func, reduce_func, apply_node_func\n\u001b[0;32m   5112\u001b[0m )\n\u001b[0;32m   5113\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5114\u001b[0m     core\u001b[39m.\u001b[39mis_builtin(reduce_func)\n\u001b[0;32m   5115\u001b[0m     \u001b[39mand\u001b[39;00m reduce_func\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   5116\u001b[0m     \u001b[39mand\u001b[39;00m ndata\n\u001b[0;32m   5117\u001b[0m ):\n\u001b[0;32m   5118\u001b[0m     \u001b[39m# Replace infinity with zero for isolated nodes\u001b[39;00m\n\u001b[0;32m   5119\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ndata\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\core.py:405\u001b[0m, in \u001b[0;36mmessage_passing\u001b[1;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m         orig_nid \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mdstdata\u001b[39m.\u001b[39mget(NID, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 405\u001b[0m         ndata \u001b[39m=\u001b[39m invoke_udf_reduce(g, rfunc, msgdata, orig_nid\u001b[39m=\u001b[39;49morig_nid)\n\u001b[0;32m    406\u001b[0m \u001b[39m# apply phase\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m afunc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\core.py:121\u001b[0m, in \u001b[0;36minvoke_udf_reduce\u001b[1;34m(graph, func, msgdata, orig_nid)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke_udf_reduce\u001b[39m(graph, func, msgdata, \u001b[39m*\u001b[39m, orig_nid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Invoke user-defined reduce function on all the nodes in the graph.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[39m    It analyzes the graph, groups nodes by their degrees and applies the UDF on each\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39m        Results from running the UDF.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     degs \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49min_degrees()\n\u001b[0;32m    122\u001b[0m     nodes \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mdstnodes()\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m orig_nid \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\heterograph.py:3671\u001b[0m, in \u001b[0;36mDGLGraph.in_degrees\u001b[1;34m(self, v, etype)\u001b[0m\n\u001b[0;32m   3669\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdstnodes(dsttype)\n\u001b[0;32m   3670\u001b[0m v_tensor \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mprepare_tensor(\u001b[39mself\u001b[39m, v, \u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 3671\u001b[0m deg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49min_degrees(etid, v_tensor)\n\u001b[0;32m   3672\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, numbers\u001b[39m.\u001b[39mIntegral):\n\u001b[0;32m   3673\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mas_scalar(deg)\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\heterograph_index.py:720\u001b[0m, in \u001b[0;36mHeteroGraphIndex.in_degrees\u001b[1;34m(self, etype, v)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39min_degrees\u001b[39m(\u001b[39mself\u001b[39m, etype, v):\n\u001b[0;32m    703\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the in degrees of the nodes.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[39m    Assume that node_type(v) == dst_type(etype). Thus, the ntype argument is omitted.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[39m        The in degree array.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mfrom_dgl_nd(\n\u001b[1;32m--> 720\u001b[0m         _CAPI_DGLHeteroInDegrees(\u001b[39mself\u001b[39;49m, \u001b[39mint\u001b[39;49m(etype), F\u001b[39m.\u001b[39;49mto_dgl_nd(v))\n\u001b[0;32m    721\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:212\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    210\u001b[0m ret_val \u001b[39m=\u001b[39m DGLValue()\n\u001b[0;32m    211\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m--> 212\u001b[0m check_call(\n\u001b[0;32m    213\u001b[0m     _LIB\u001b[39m.\u001b[39;49mDGLFuncCall(\n\u001b[0;32m    214\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m    215\u001b[0m         values,\n\u001b[0;32m    216\u001b[0m         tcodes,\n\u001b[0;32m    217\u001b[0m         ctypes\u001b[39m.\u001b[39;49mc_int(num_args),\n\u001b[0;32m    218\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(ret_val),\n\u001b[0;32m    219\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(ret_tcode),\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m )\n\u001b[0;32m    222\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[0;32m    223\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "File \u001b[1;32mc:\\Users\\yuki_younai\\.conda\\envs\\PyTorch\\lib\\site-packages\\dgl\\_ffi\\base.py:70\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(py_str(_LIB\u001b[39m.\u001b[39mDGLGetLastError()))\n",
      "\u001b[1;31mDGLError\u001b[0m: [09:17:32] C:\\Users\\Administrator\\DGL_scripts\\release\\win-64\\dgl\\src\\runtime\\cuda\\cuda_device_api.cc:117: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "loss_train = torch.zeros(1).to(device)\n",
    "\n",
    "graph_pos = dataloader.train_graph\n",
    "for i in range(args.neg_number):\n",
    "    graph_neg = construct_negative_graph(graph_pos, ('user', 'rate', 'item'))\n",
    "    loss_moe=0\n",
    "    if args.moe:\n",
    "            score_pos, score_neg,loss_moe = model(graph_pos, graph_neg)\n",
    "    else:\n",
    "            score_pos, score_neg = model(graph_pos, graph_neg)\n",
    "            \n",
    "    if not args.category_balance:\n",
    "        loss_train += -(score_pos - score_neg).sigmoid().log().mean()+0.2*loss_moe\n",
    "    else:\n",
    "        loss = -(score_pos - score_neg).sigmoid().log()\n",
    "        items = graph_pos.edges(etype = 'rate')[1]\n",
    "        weight = sample_weight[items]\n",
    "        loss_train += (weight * loss.squeeze(1)).mean()+0.2*loss_moe\n",
    "        print((weight * loss.squeeze(1)).mean(),loss_moe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
